---
title: "ARE 212 Midterm"
author: "Anaya Hall"
date: "March 14, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(readr)
library(magrittr)
```

# Question One: â€œLinkages among climate change, crop yields and Mexico - US cross-border migration"

Load OLS functions

```{r OLD OLS function, include=FALSE}
ols <- function(data, y_data, X_data, intercept = T, H0 = 0, two_tail = T, alpha = 0.05) {
  # Function setup ----
    # Require the 'dplyr' package
    require(dplyr)
    # Function to convert tibble, data.frame, or tbl_df to matrix
    to_matrix <- function(the_df, vars) {
      # Create a matrix from variables in var
      new_mat <- the_df %>%
        #Select the columns given in 'vars'
        select_(.dots = vars) %>%
        # Convert to matrix
        as.matrix()
      # Return 'new_mat'
      return(new_mat)
    }
  
  # Create dependent and independent variable matrices ----
    # y matrix
    y <- to_matrix (the_df = data, vars = y_data)
    # X matrix
    X <- to_matrix (the_df = data, vars = X_data)
      # If 'intercept' is TRUE, then add a column of ones
      if (intercept == T) {
      X <- cbind(1,X)
      colnames(X) <- c("intercept", X_data)
      }
 
  # Calculate b, y_hat, and residuals ----
    b <- solve(t(X) %*% X) %*% t(X) %*% y
    y_hat <- X %*% b
    e <- y - y_hat
  
  # Useful -----
    n <- nrow(X) # number of observations
    k <- ncol(X) # number of independent variables
    dof <- n - k # degrees of freedom
    i <- rep(1,n) # column of ones for demeaning matrix
    A <- diag(i) - (1 / n) * i %*% t(i) # demeaning matrix
    y_star <- A %*% y # for SST
    X_star <- A %*% X # for SSM
    SST <- drop(t(y_star) %*% y_star)
    SSM <- drop(t(b) %*% t(X_star) %*% X_star %*% b)
    SSR <- drop(t(e) %*% e)
  
  # Measures of fit and estimated variance ----
    R2uc <- drop((t(y_hat) %*% y_hat)/(t(y) %*% y)) # Uncentered R^2
    R2 <- 1 - SSR/SST # Uncentered R^2
    R2adj <- 1 - (n-1)/dof * (1 - R2) # Adjusted R^2
    AIC <- log(SSR/n) + 2*k/n # AIC
    SIC <- log(SSR/n) + k/n*log(n) # SIC
    s2 <- SSR/dof # s^2
  
  # Measures of fit table ----
    mof_table_df <- data.frame(R2uc, R2, R2adj, SIC, AIC, SSR, s2)
    mof_table_col_names <- c("$R^2_\\text{uc}$", "$R^2$",
                             "$R^2_\\text{adj}$",
                             "SIC", "AIC", "SSR", "$s^2$")
    mof_table <-  mof_table_df %>% knitr::kable(
      row.names = F,
      col.names = mof_table_col_names,
      format.args = list(scientific = F, digits = 4),
      booktabs = T,
      escape = F
    )
  
  # t-test----
    # Standard error
    se <- as.vector(sqrt(s2 * diag(solve(t(X) %*% X))))
    # Vector of _t_ statistics
    t_stats <- (b - H0) / se
    # Calculate the p-values
    if (two_tail == T) {
    p_values <- pt(q = abs(t_stats), df = dof, lower.tail = F) * 2
    } else {
      p_values <- pt(q = abs(t_stats), df = dof, lower.tail = F)
    }
    # Do we (fail to) reject?
    reject <- ifelse(p_values < alpha, reject <- "Reject", reject <- "Fail to Reject")
    
    # Nice table (data.frame) of results
    ttest_df <- data.frame(
      # The rows have the coef. names
      effect = rownames(b),
      # Estimated coefficients
      coef = as.vector(b) %>% round(3),
      # Standard errors
      std_error = as.vector(se) %>% round(3),
      # t statistics
      t_stat = as.vector(t_stats) %>% round(3),
      # p-values
      p_value = as.vector(p_values) %>% round(4),
      # reject null?
      significance = as.character(reject)
      )
  
    ttest_table <-  ttest_df %>% knitr::kable(
      booktabs = T,
      format.args = list(scientific = F),
      escape = F
    )

  # Data frame for exporting for y, y_hat, X, and e vectors ----
    export_df <- data.frame(y, y_hat, e, X) %>% tbl_df()
    colnames(export_df) <- c("y","y_hat","e",colnames(X))
  
  # Return ----
    return(list(n=n, dof=dof, b=b, vars=export_df, R2uc=R2uc,R2=R2,
                R2adj=R2adj, AIC=AIC, SIC=SIC, s2=s2, SST=SST, SSR=SSR,
                mof_table=mof_table, ttest=ttest_table))
}
```

```{r OLS functions}
# Function to turn given data into matrix for use in OLS function
to_matrix <- function(the_df, vars) {
  # Create a matrix from variables in var
  new_mat <- the_df %>%
    # Select the columns given in 'vars'
    select_(.dots = vars) %>%
    # Convert to matrix
    as.matrix()
  # Return 'new_mat'
  return(new_mat)
}

# Function for OLS coefficient estimates and measures of fit
b_ols <- function(data, y_data, X_data, intercept=TRUE) {

  require(dplyr)  
    # y matrix
    y <- to_matrix (the_df = data, vars = y_data)
    # X matrix
    X <- to_matrix (the_df = data, vars = X_data)
      # If 'intercept' is TRUE, then add a column of ones
      if (intercept == T) {
        X <- cbind(1,X)
        colnames(X) <- c("intercept", X_data)
      }

    # Calculate beta hat ------
    b <- solve( t(X) %*% X ) %*% t(X) %*% y
    # Change the name of 'ones' to 'intercept'
    if(intercept == T){
        rownames(b) <- c("intercept", X_data) }
    else
        rownames(b) <- c(X_data)
    
    y_hat <- X %*% b
    e <- y - y_hat
  
    # Useful transformations -----
    n <- nrow(X) # number of observations
    k <- ncol(X) # number of independent variables
    dof <- n - k # degrees of freedom
    i <- rep(1,n) # column of ones for demeaning matrix
    A <- diag(i) - (1 / n) * i %*% t(i) # demeaning matrix
    M <- diag(i) - (X) %*% 
    y_star <- A %*% y # for SST
    X_star <- A %*% X # for SSM
    SST <- drop(t(y_star) %*% y_star)
    SSM <- drop(t(b) %*% t(X_star) %*% X_star %*% b)
    SSR <- drop(t(e) %*% e)
  
    # Measures of fit and estimated variance ----
    R2uc <- drop((t(y_hat) %*% y_hat)/(t(y) %*% y)) # Uncentered R^2
    R2 <- 1 - SSR/SST # Uncentered R^2
    R2adj <- 1 - (n-1)/dof * (1 - R2) # Adjusted R^2
    AIC <- log(SSR/n) + 2*k/n # AIC
    SIC <- log(SSR/n) + k/n*log(n) # SIC
    s2 <- SSR/dof # s^2
  
    results <- data.frame(
      # The rows have the coef. names
      x_var = rownames(b),
      # Estimated coefficients
      coef = as.vector(b) %>% round(3)
    )

      # Return beta_hat & adjusted r2
    #return(R2adj)
    return(results)
}

```

```{r auto_data, include=FALSE}
#load test data
auto <- read_csv("~/projects/are212/section/Section01/auto.csv")
```


```{r}
# check ols function
b_ols(data = auto, y_data = "price", X_data = c("mpg", "headroom"))
```


Load & clean data

```{r load_data}
feng <- read_csv()



```
Inspect data
```{r}

```
1. **Estimate model (1) via OLS by regressing emigration rate on log of yields and a time period fixed effect. Report coefficient on yield and adjusted $R^2$. Does this match the results in the first column of table #1?**

```{r}
b_ols(data = auto, y_data = "price", X_data = c("mpg", "headroom"))
```


2. Estimate model (1) again via fixed effects and FWT. Report coefficient on yield and adjusted $R^2$. Does this match the results in the third column of table #1?

```{r FWT_pset1}

#Step one: regress {per capita} CO$_2$ emissions on \textit{per capita} GDP and save the residuals.
# Step 1 Residuals
step1_resid <- resid_ols(wdi, "co2pc", "gdppc", F)


#Step two: create a column of ones; then regress $[i\quad\mathbf{GDPpc2}]$ on $\mathbf{GDPpc}$.

# Create a column of ones
wdi %<>% mutate(ones = 1)
# Our two regressions
step2a_resid <- resid_ols(wdi, "ones", "gdppc", F)
step2b_resid <- resid_ols(wdi, "gdppc2", "gdppc", F)

#Step three: build a dataset of residuals; regress the residuals from step one on the residuals from step two.
# Build the dataset of residualized variables
df19 <- data.frame(
  co2pc_resid  = as.vector(step1_resid),
  i_resid      = as.vector(step2a_resid),
  gdppc2_resid = as.vector(step2b_resid)
  ) %>% tbl_df()
# The final regression
b_ols(df19, "co2pc_resid", c("i_resid", "gdppc2_resid"), F)

```


3. Repeat step 1 without the the fixed effects. Report coefficient on yield and adjusted $R^2$. Do the results look different from what you estimated before? From what is in the paper?

4. Repeat step 2 without the the fixed effects. Report coefficient on yield and adjusted $R^2$. Do the results look different from what you estimated before? From what is in the paper?

5. What happened here? What are the consequences?


```{r}


```


# Question Two: Normality of OLS

Model:
$y_i$ = $\beta_o$ + $\beta_1$$x_{1i}$ + $\beta_2$$x_{2i}$ + $\epsilon_i$

Truth:
$\beta_0$ = 3 , $\beta_1$ = 1, $\beta_2$ = -2

###Load functions for use in simulation

Generate data function (given a sample size, n)
```{r gen_data function}

#ADD SWEEP TO FIX COVARIANCE OF X1 & X2
gen_data <- function(sample_size) {
  # Create data.frame with random x and error
  data_df <- data.frame(
    X1 = rnorm(n = sample_size, sd = 5),
    X2 = rnorm(n = sample_size, sd = 5),
    # X <- rnorm(n = sample_size, sd =5) %>% matrix(sample_size, 2),
    e = rnorm(sample_size, sd = 5),
    eta = runif(sample_size, -8.66, 8.66))
  # Calculate y = 3 + 1 x1 - 2 x2 + e; drop 'e'
  data_df %<>% mutate(y_a = 3 + 1 * X1 - 2 * X2 + e, 
                      y_b = 3 + 1 * X1 - 2 * X2 + eta) %>%
    select(-e, -eta)
  # Return data_df
  return(data_df)
  
}

 # test <- gen_data(10)
 # test
```

Function for a single simulation of OLS
```{r one_sim function}
one_sim <- function(sample_size, depvar) {
  # Estimate via OLS
  ols_est <- b_ols(data = gen_data(sample_size),
    y_data = depvar, X_data = c("X1", "X2"))
  # Grab the estimated coefficient on x
  # (the second element of 'coef')
  b2 <- ols_est %$% coef[3]
  # Return a data.frame with b2
  return(data.frame(b2))
}
```


Function for multiple simulations of OLS
```{r}
ols_sim <- function(depvar, n_sims = 1e4, sample_size, seed = 22092008) {
  # Set the seed
  set.seed(seed)
  # Run one_sim n_sims times; convert results to data.frame
  sim_df <- replicate(
    n = n_sims,
    expr = one_sim(sample_size, depvar),
    simplify = F
    ) %>% bind_rows()
  # Return sim_df
  return(sim_df)
}
```

For each part, repeat for sample sizes: n=[10, 100, 1000, 10000, 20000] and run 1e4 simulations




###Part A: 
Regress $y^a$ on intercept, $x_1$ and $x_2$. Record $\beta_2$ 
```{r ya_sims}
N <- c(10, 100, 1000, 10000, 20000)

for (n in N){
 print(n)
  # sim_A <- matrix()
  # sim10 <- ols_sim(depvar = "y_a", n_sims = 1e4, sample_size = n)

}
# Run for sample sizes: n=[10, 100, 1000, 10000, 20000]
# # Run ols_sim for sample size of 10
# sim10 <- ols_sim(depvar = "y_a", n_sims = 1e4, sample_size = 10)
# # Run ols_sim for sample size of 100
# sim100 <- ols_sim(depvar = "y_a", n_sims = 1e4, sample_size = 100)
# # Run ols_sim for sample size of 1000
# sim1000 <- ols_sim(depvar = "y_a", n_sims = 1e4, sample_size = 1000)
# # Run ols_sim for sample size of 10000
# sim10000 <- ols_sim(depvar = "y_a", n_sims = 1e4, sample_size = 10000)
# # Run ols_sim for sample size of 20000
# sim20000 <- ols_sim(depvar = "y_a", n_sims = 1e4, sample_size = 20000)

```

Plot histogram

###Part B:
Regress $y^b$ on $x_1$ and $x_2$. Record $\beta_2$ 



Plot histogram